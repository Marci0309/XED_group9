Running LoRA fine-tuning on a100gpu5
Using device: cuda

==============================
 Fine-tuning model: mistralai/Mistral-7B-Instruct-v0.1
==============================
trainable params: 13,631,488 || all params: 7,255,363,584 || trainable%: 0.1879
{'loss': 3.2217, 'grad_norm': 1255.9114990234375, 'learning_rate': 0.00016610169491525423, 'epoch': 0.13}
{'loss': 2.2701, 'grad_norm': 1.5955870151519775, 'learning_rate': 0.00019936784251565942, 'epoch': 0.26}
{'loss': 2.0688, 'grad_norm': 1.318985939025879, 'learning_rate': 0.00019681339461424585, 'epoch': 0.38}
{'loss': 2.0286, 'grad_norm': 1.609708547592163, 'learning_rate': 0.00019234754034237907, 'epoch': 0.51}
{'eval_loss': 1.9937455654144287, 'eval_runtime': 72.8847, 'eval_samples_per_second': 10.743, 'eval_steps_per_second': 10.743, 'epoch': 0.51}
{'loss': 2.0054, 'grad_norm': 1.2459927797317505, 'learning_rate': 0.00018605844996140322, 'epoch': 0.64}
{'loss': 1.9762, 'grad_norm': 1.3167847394943237, 'learning_rate': 0.0001780702902507857, 'epoch': 0.77}
{'loss': 1.9628, 'grad_norm': 1.5112444162368774, 'learning_rate': 0.00016854077305835413, 'epoch': 0.89}
{'loss': 1.9307, 'grad_norm': 1.8422884941101074, 'learning_rate': 0.000157658041563485, 'epoch': 1.02}
{'eval_loss': 1.9263112545013428, 'eval_runtime': 76.9229, 'eval_samples_per_second': 10.179, 'eval_steps_per_second': 10.179, 'epoch': 1.02}
{'loss': 1.7422, 'grad_norm': 1.687972903251648, 'learning_rate': 0.0001456369557283778, 'epoch': 1.15}
{'loss': 1.7097, 'grad_norm': 2.00710391998291, 'learning_rate': 0.00013271485027449895, 'epoch': 1.28}
{'loss': 1.7108, 'grad_norm': 2.095487117767334, 'learning_rate': 0.00011914684893532182, 'epoch': 1.4}
{'loss': 1.7204, 'grad_norm': 1.5717028379440308, 'learning_rate': 0.00010520082749701581, 'epoch': 1.53}
{'eval_loss': 1.8648710250854492, 'eval_runtime': 75.5975, 'eval_samples_per_second': 10.357, 'eval_steps_per_second': 10.357, 'epoch': 1.53}
{'loss': 1.6686, 'grad_norm': 2.0630033016204834, 'learning_rate': 9.115212507278798e-05, 'epoch': 1.66}
{'loss': 1.6715, 'grad_norm': 1.9848284721374512, 'learning_rate': 7.727810802725428e-05, 'epoch': 1.79}
{'loss': 1.6441, 'grad_norm': 2.219233989715576, 'learning_rate': 6.385269387637687e-05, 'epoch': 1.91}
{'loss': 1.5611, 'grad_norm': 3.104355573654175, 'learning_rate': 5.114094327871663e-05, 'epoch': 2.04}
{'eval_loss': 1.8978484869003296, 'eval_runtime': 72.1919, 'eval_samples_per_second': 10.846, 'eval_steps_per_second': 10.846, 'epoch': 2.04}
{'loss': 1.3488, 'grad_norm': 2.5834171772003174, 'learning_rate': 3.9393826889406536e-05, 'epoch': 2.17}
{'loss': 1.3662, 'grad_norm': 3.366950750350952, 'learning_rate': 2.8843270395903776e-05, 'epoch': 2.3}
{'loss': 1.3462, 'grad_norm': 2.676635980606079, 'learning_rate': 1.9697575562379212e-05, 'epoch': 2.42}
{'loss': 1.3498, 'grad_norm': 2.6576569080352783, 'learning_rate': 1.2137307685986931e-05, 'epoch': 2.55}
{'eval_loss': 1.8532795906066895, 'eval_runtime': 76.0059, 'eval_samples_per_second': 10.302, 'eval_steps_per_second': 10.302, 'epoch': 2.55}
{'loss': 1.3358, 'grad_norm': 2.876800537109375, 'learning_rate': 6.311730659795123e-06, 'epoch': 2.68}
{'loss': 1.3156, 'grad_norm': 2.8123908042907715, 'learning_rate': 2.3358600256489836e-06, 'epoch': 2.81}
{'loss': 1.3386, 'grad_norm': 2.6112020015716553, 'learning_rate': 2.881921991373382e-07, 'epoch': 2.93}
{'train_runtime': 6372.9728, 'train_samples_per_second': 2.95, 'train_steps_per_second': 0.185, 'train_loss': 1.7430428323291598, 'epoch': 3.0}

 Test Results for mistral_7b: {'eval_loss': 1.840938925743103, 'eval_runtime': 75.1711, 'eval_samples_per_second': 10.43, 'eval_steps_per_second': 10.43, 'epoch': 3.0}

==============================
 Fine-tuning model: mistralai/Mixtral-8x7B-Instruct-v0.1
==============================

###############################################################################
H치br칩k Cluster
Job 24933056 for user s5542332
Finished at: Mon Oct 27 17:37:47 CET 2025

Job details:
============

Job ID                         : 24933056
Name                           : lora_mistral
User                           : s5542332
Partition                      : gpumedium
Nodes                          : a100gpu5
Number of Nodes                : 1
Cores                          : 8
Number of Tasks                : 1
State                          : FAILED  
Submit                         : 2025-10-27T14:04:22
Start                          : 2025-10-27T15:48:33
End                            : 2025-10-27T17:37:42
Reserved walltime              : 1-00:00:00
Used walltime                  :   01:49:09
Used CPU time                  :   01:50:02 (Efficiency: 12.60%)
% User (Computation)           : 98.83%
% System (I/O)                 :  1.16%
Total memory reserved          : 32G
Maximum memory used            : 22.43G
Requested GPUs                 : a100=1
Allocated GPUs                 : a100=1
Max GPU utilization            : 83%
Max GPU memory used            : 7.40G

Acknowledgements:
=================

Please see this page for information about acknowledging H치br칩k in your publications:

https://wiki.hpc.rug.nl/habrok/introduction/scientific_output

################################################################################
