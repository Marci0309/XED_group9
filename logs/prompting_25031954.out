Running LoRA fine-tuning on v100v2gpu5
================================================================================

[1;36m                             Loading configuration                              [0m

================================================================================
================================================================================

[1;32m                             Configuration settings                             [0m

================================================================================
EMOTION_ANALYSIS:
  ENABLE_PLOTTING: true
  LANGUAGES:
  - en
  - nl
  - hu
  - ro
GENERAL:
  DATA_DIR: '!OriginalData'
  MODELS_DIR: models
  OUTPUT_DIR: data
  PLOTS_DIR: plots
  SEED: 42
  TEST_SIZE: 0.2
LORA:
  BATCH_SIZE: 1
  DEVICE_IDS: '0'
  GRAD_ACCUM_STEPS: 16
  LEARNING_RATE: 5e-4
  LORA_ALPHA: 32
  LORA_DROPOUT: 0.05
  LORA_FINETUNE: false
  LORA_MODELS:
    ministral_3b: mistralai/Ministral-3B-v0.1
    mistral_tiny: mistralai/Mistral-3B-Instruct-v0.1
  LORA_RANK: 16
  NUM_EPOCHS: 4
PROMPTING:
  DEVICE: null
  ENABLE_PROMPTING: true
  FEW_SHOT_EXAMPLES:
  - label: surprise
    text: I can't believe this happened!
  - label: joy, trust
    text: I'm so happy for you!
  - label: anger
    text: This is so unfair.
  FEW_SHOT_INTRO: 'You are an emotion classifier. Here are examples:

    '
  INSTRUCTION_TEXT: 'You are a precise emotion classifier. Identify all emotions in
    the sentence based on Plutchik''s 8 basic emotions and return them as comma-separated
    values from: joy, trust, fear, surprise, sadness, disgust, anger, anticipation.

    '
  MAX_NEW_TOKENS: 50
  MODEL_NAME: mistralai/Mistral-7B-Instruct-v0.1

================================================================================

[1;32m                             Analyzing emotion data                             [0m

================================================================================
Loaded dataset: !OriginalData/en-projections.tsv (7834 rows)
Counted 8 unique emotions
Saved plot to: plots/emotion_distribution_en.png
Loaded dataset: !OriginalData/nl-projections.tsv (5334 rows)
Counted 8 unique emotions
Saved plot to: plots/emotion_distribution_nl.png
Loaded dataset: !OriginalData/hu-projections.tsv (5778 rows)
Counted 8 unique emotions
Saved plot to: plots/emotion_distribution_hu.png
Loaded dataset: !OriginalData/ro-projections.tsv (9475 rows)
Counted 8 unique emotions
Saved plot to: plots/emotion_distribution_ro.png
Saved emotion distribution plots to 'plots/'
================================================================================

[1;36m                           Emotion Data Preprocessing                           [0m

================================================================================
Processing en from !OriginalData/en-projections.tsv ...
  Saved train â†’ data/en/train.jsonl
  Saved validation â†’ data/en/validation.jsonl
  Saved test â†’ data/en/test.jsonl
Processing hu from !OriginalData/hu-projections.tsv ...
  Saved train â†’ data/hu/train.jsonl
  Saved validation â†’ data/hu/validation.jsonl
  Saved test â†’ data/hu/test.jsonl
Processing nl from !OriginalData/nl-projections.tsv ...
  Saved train â†’ data/nl/train.jsonl
  Saved validation â†’ data/nl/validation.jsonl
  Saved test â†’ data/nl/test.jsonl
Processing ro from !OriginalData/ro-projections.tsv ...
  Saved train â†’ data/ro/train.jsonl
  Saved validation â†’ data/ro/validation.jsonl
  Saved test â†’ data/ro/test.jsonl
================================================================================

[1;32m                             Preprocessing Complete                             [0m

================================================================================
================================================================================

[1;31m                    LoRA fine-tuning disabled in config.yaml                    [0m

================================================================================
================================================================================

[1;36m                        Evaluating Prompting Strategies                         [0m

================================================================================
Using device: cuda
Loading model: mistralai/Mistral-7B-Instruct-v0.1

Loading test datasets...

Evaluating on single-language dataset...

Evaluating strategy: zero-shot on 784 samples

###############################################################################
HÃ¡brÃ³k Cluster
Job 25031954 for user s5542332
Finished at: Thu Oct 30 17:46:48 CET 2025

Job details:
============

Job ID                         : 25031954
Name                           : prompting
User                           : s5542332
Partition                      : gpumedium
Nodes                          : v100v2gpu5
Number of Nodes                : 1
Cores                          : 8
Number of Tasks                : 1
State                          : FAILED  
Submit                         : 2025-10-30T17:43:50
Start                          : 2025-10-30T17:45:37
End                            : 2025-10-30T17:46:44
Reserved walltime              : 08:00:00
Used walltime                  : 00:01:07
Used CPU time                  : 00:00:25 (Efficiency:  4.72%)
% User (Computation)           : 68.54%
% System (I/O)                 : 31.45%
Total memory reserved          : 32G
Maximum memory used            : 14.93G
Requested GPUs                 : 1
Allocated GPUs                 : v100=1
Max GPU utilization            : 4%
Max GPU memory used            : 14.04G

Acknowledgements:
=================

Please see this page for information about acknowledging HÃ¡brÃ³k in your publications:

https://wiki.hpc.rug.nl/habrok/introduction/scientific_output

################################################################################
