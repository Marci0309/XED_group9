Running LoRA fine-tuning on v100v2gpu12
Using device: cuda
Loading model: mistralai/Mistral-7B-Instruct-v0.1

 Loading test datasets...

 Evaluating on single-language dataset...

 Evaluating strategy: zero-shot on 784 samples
Accuracy (zero-shot): 0.008

 Evaluating strategy: few-shot on 784 samples
Accuracy (few-shot): 0.293

 Evaluating strategy: instruction on 784 samples
Accuracy (instruction): 0.694

 Evaluating on multi-language dataset...

 Evaluating strategy: zero-shot on 2843 samples
Accuracy (zero-shot): 0.014

 Evaluating strategy: few-shot on 2843 samples
Accuracy (few-shot): 0.288

 Evaluating strategy: instruction on 2843 samples

###############################################################################
H치br칩k Cluster
Job 24964901 for user s5542332
Finished at: Tue Oct 28 17:24:29 CET 2025

Job details:
============

Job ID                         : 24964901
Name                           : prompting
User                           : s5542332
Partition                      : gpumedium
Nodes                          : v100v2gpu12
Number of Nodes                : 1
Cores                          : 8
Number of Tasks                : 1
State                          : TIMEOUT  
Submit                         : 2025-10-28T12:23:02
Start                          : 2025-10-28T12:24:17
End                            : 2025-10-28T17:24:24
Reserved walltime              : 05:00:00
Used walltime                  : 05:00:07
Used CPU time                  : 04:58:12 (Efficiency: 12.42%)
% User (Computation)           : 97.41%
% System (I/O)                 :  2.59%
Total memory reserved          : 32G
Maximum memory used            : 15.42G
Requested GPUs                 : 1
Allocated GPUs                 : v100=1
Max GPU utilization            : 100%
Max GPU memory used            : 23.40G

Acknowledgements:
=================

Please see this page for information about acknowledging H치br칩k in your publications:

https://wiki.hpc.rug.nl/habrok/introduction/scientific_output

################################################################################
