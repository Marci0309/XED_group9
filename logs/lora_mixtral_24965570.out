Running LoRA fine-tuning on a100gpu5
Using device: cuda

==============================
 Fine-tuning model: mistralai/Mixtral-8x7B-Instruct-v0.1
==============================

###############################################################################
H치br칩k Cluster
Job 24965570 for user s5542332
Finished at: Tue Oct 28 14:10:11 CET 2025

Job details:
============

Job ID                         : 24965570
Name                           : lora_mixtral
User                           : s5542332
Partition                      : gpumedium
Nodes                          : a100gpu5
Number of Nodes                : 1
Cores                          : 8
Number of Tasks                : 1
State                          : FAILED  
Submit                         : 2025-10-28T12:36:01
Start                          : 2025-10-28T14:09:00
End                            : 2025-10-28T14:10:06
Reserved walltime              : 1-00:00:00
Used walltime                  :   00:01:06
Used CPU time                  :   00:01:23 (Efficiency: 15.69%)
% User (Computation)           : 68.68%
% System (I/O)                 : 31.32%
Total memory reserved          : 32G
Maximum memory used            : 7.17G
Requested GPUs                 : a100=1
Allocated GPUs                 : a100=1
Max GPU utilization            : 0%
Max GPU memory used            : 0.00 

Acknowledgements:
=================

Please see this page for information about acknowledging H치br칩k in your publications:

https://wiki.hpc.rug.nl/habrok/introduction/scientific_output

################################################################################
