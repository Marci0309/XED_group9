Running LoRA fine-tuning on a100gpu3
================================================================================

[1;36m                             Loading configuration                              [0m

================================================================================
================================================================================

[1;32m                             Configuration settings                             [0m

================================================================================
EMOTION_ANALYSIS:
  ENABLE_PLOTTING: true
  LANGUAGES:
  - en
  - nl
  - hu
  - ro
GENERAL:
  DATA_DIR: '!OriginalData'
  MODELS_DIR: models
  OUTPUT_DIR: data
  PLOTS_DIR: plots
  SEED: 42
  TEST_SIZE: 0.2
LORA:
  BATCH_SIZE: 1
  DEVICE_IDS: '0'
  GRAD_ACCUM_STEPS: 16
  LEARNING_RATE: 5e-4
  LORA_ALPHA: 32
  LORA_DROPOUT: 0.05
  LORA_FINETUNE: false
  LORA_MODELS:
    ministral_3b: mistralai/Ministral-3B-v0.1
    mistral_tiny: mistralai/Mistral-3B-Instruct-v0.1
  LORA_RANK: 16
  NUM_EPOCHS: 4
PROMPTING:
  DEVICE: null
  ENABLE_PROMPTING: true
  FEW_SHOT_EXAMPLES:
  - label: surprise
    text: I can't believe this happened!
  - label: joy, trust
    text: I'm so happy for you!
  - label: anger
    text: This is so unfair.
  FEW_SHOT_INTRO: 'You are an emotion classifier. Here are examples:

    '
  INSTRUCTION_TEXT: 'You are a precise emotion classifier. Identify all emotions in
    the sentence based on Plutchik''s 8 basic emotions and return them as comma-separated
    values from: joy, trust, fear, surprise, sadness, disgust, anger, anticipation.

    '
  MAX_NEW_TOKENS: 64
  MODEL_NAME: mistralai/Mistral-7B-Instruct-v0.1

================================================================================

[1;32m                             Analyzing emotion data                             [0m

================================================================================
Loaded dataset: !OriginalData/en-projections.tsv (7834 rows)
Counted 8 unique emotions
Saved plot to: plots/emotion_distribution_en.png
Loaded dataset: !OriginalData/nl-projections.tsv (5334 rows)
Counted 8 unique emotions
Saved plot to: plots/emotion_distribution_nl.png
Loaded dataset: !OriginalData/hu-projections.tsv (5778 rows)
Counted 8 unique emotions
Saved plot to: plots/emotion_distribution_hu.png
Loaded dataset: !OriginalData/ro-projections.tsv (9475 rows)
Counted 8 unique emotions
Saved plot to: plots/emotion_distribution_ro.png
Saved emotion distribution plots to 'plots/'
================================================================================

[1;36m                           Emotion Data Preprocessing                           [0m

================================================================================
Processing en from !OriginalData/en-projections.tsv ...
  Saved train â†’ data/en/train.jsonl
  Saved validation â†’ data/en/validation.jsonl
  Saved test â†’ data/en/test.jsonl
Processing hu from !OriginalData/hu-projections.tsv ...
  Saved train â†’ data/hu/train.jsonl
  Saved validation â†’ data/hu/validation.jsonl
  Saved test â†’ data/hu/test.jsonl
Processing nl from !OriginalData/nl-projections.tsv ...
  Saved train â†’ data/nl/train.jsonl
  Saved validation â†’ data/nl/validation.jsonl
  Saved test â†’ data/nl/test.jsonl
Processing ro from !OriginalData/ro-projections.tsv ...
  Saved train â†’ data/ro/train.jsonl
  Saved validation â†’ data/ro/validation.jsonl
  Saved test â†’ data/ro/test.jsonl
================================================================================

[1;32m                             Preprocessing Complete                             [0m

================================================================================
================================================================================

[1;31m                    LoRA fine-tuning disabled in config.yaml                    [0m

================================================================================
================================================================================

[1;36m                        Evaluating Prompting Strategies                         [0m

================================================================================
Using device: cuda
Loading model: mistralai/Mistral-7B-Instruct-v0.1

Loading test datasets...

Evaluating on single-language dataset...

Evaluating strategy: zero-shot on 784 samples
Accuracy (zero-shot): 0.031

Evaluating strategy: few-shot on 784 samples
Accuracy (few-shot): 0.719

Evaluating strategy: instruction on 784 samples
Accuracy (instruction): 1.000

Evaluating on multi-language dataset...

Evaluating strategy: zero-shot on 2843 samples
Accuracy (zero-shot): 0.036

Evaluating strategy: few-shot on 2843 samples
Accuracy (few-shot): 0.711

Evaluating strategy: instruction on 2843 samples
Accuracy (instruction): 1.000

===== Final Summary =====
single_zero: 0.031
single_few: 0.719
single_instruct: 1.000
multi_zero: 0.036
multi_few: 0.711
multi_instruct: 1.000

Results saved to: models/prompting_results.json
================================================================================

[1;32m                         Prompting Evaluation Complete                          [0m

================================================================================

###############################################################################
HÃ¡brÃ³k Cluster
Job 25076806 for user s5542332
Finished at: Sun Nov  2 13:15:30 CET 2025

Job details:
============

Job ID                         : 25076806
Name                           : lora_mixtral
User                           : s5542332
Partition                      : gpumedium
Nodes                          : a100gpu3
Number of Nodes                : 1
Cores                          : 8
Number of Tasks                : 1
State                          : COMPLETED  
Submit                         : 2025-11-02T10:11:05
Start                          : 2025-11-02T10:11:06
End                            : 2025-11-02T13:15:26
Reserved walltime              : 1-00:00:00
Used walltime                  :   03:04:20
Used CPU time                  :   03:03:16 (Efficiency: 12.43%)
% User (Computation)           : 97.69%
% System (I/O)                 :  2.32%
Total memory reserved          : 32G
Maximum memory used            : 1.37G
Requested GPUs                 : 1
Allocated GPUs                 : a100=1
Max GPU utilization            : 100%
Max GPU memory used            : 24.94G

Acknowledgements:
=================

Please see this page for information about acknowledging HÃ¡brÃ³k in your publications:

https://wiki.hpc.rug.nl/habrok/introduction/scientific_output

################################################################################
